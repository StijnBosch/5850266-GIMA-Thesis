{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e564c7c1",
   "metadata": {},
   "source": [
    "## Import relevant modules, data and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569dffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio as rs\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from osgeo import gdal, gdalconst\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd8b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Image\n",
    "def read_data_EMP(inras):\n",
    "    # Read data\n",
    "    img = gdal.Open(inras, gdal.GA_ReadOnly) \n",
    "    bands = [img.GetRasterBand(i).ReadAsArray() for i in range(1, img.RasterCount + 1)]\n",
    "    img = np.array(bands)\n",
    "    img = img[0:12,:,:]\n",
    "    img = np.transpose(img, [1, 2, 0])\n",
    "    img_rgbnir = img[:,:,[3,2,1,7]]\n",
    "    return img_rgbnir\n",
    "def MyNormalize(img_i,sigma):\n",
    "        nr,nc,nb = img_i.shape\n",
    "        img_n = np.zeros(shape=(nr,nc,nb))\n",
    "        for i in range(0,nb):\n",
    "            one_band = img_i[:,:,i]\n",
    "            mi = np.min(one_band)\n",
    "            ma = np.max(one_band)\n",
    "            one_band = (one_band-mi)/(ma-mi+np.finfo(float).eps)\n",
    "            img_n[:,:,i] = sigma*one_band\n",
    "        return img_n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3ad9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Imagery\n",
    "asi = 'Input/Imagery/SingleDateImage_Amsterdam.tif'\n",
    "ami = 'Input/Imagery/MedianImage_Amsterdam.tif'\n",
    "msi = 'Input/Imagery/SingleDateImage_Milano.tif'\n",
    "mmi = 'Input/Imagery/MedianImage_Milano.tif'\n",
    "bsi = 'Input/Imagery/SingleDateImage_Budapest.tif'\n",
    "bmi = 'Input/Imagery/MedianImage_Budapest.tif'\n",
    "#Import Feature extraction\n",
    "EMPas = np.load('D:/Thesis/Notebooks/Output/Full/EMP/empasi.npy')\n",
    "NDWIas = loadtxt('D:/Thesis/Notebooks/Output/Full/NDWI/ndwiasi.csv', delimiter=',')\n",
    "NDVIas = loadtxt('D:/Thesis/Notebooks/Output/Full/NDVI/ndviasi.csv', delimiter=',')\n",
    "EMPam = np.load('D:/Thesis/Notebooks/Output/Full/EMP/empami.npy')\n",
    "NDWIam = loadtxt('D:/Thesis/Notebooks/Output/Full/NDWI/ndwiami.csv', delimiter=',')\n",
    "NDVIam = loadtxt('D:/Thesis/Notebooks/Output/Full/NDVI/ndviami.csv', delimiter=',')\n",
    "EMPms = np.load('D:/Thesis/Notebooks/Output/Full/EMP/empmsi.npy')\n",
    "NDWIms = loadtxt('D:/Thesis/Notebooks/Output/Full/NDWI/ndwimsi.csv', delimiter=',')\n",
    "NDVIms = loadtxt('D:/Thesis/Notebooks/Output/Full/NDVI/ndvimsi.csv', delimiter=',')\n",
    "EMPmm = np.load('D:/Thesis/Notebooks/Output/Full/EMP/empmmi.npy')\n",
    "NDWImm = loadtxt('D:/Thesis/Notebooks/Output/Full/NDWI/ndwimmi.csv', delimiter=',')\n",
    "NDVImm = loadtxt('D:/Thesis/Notebooks/Output/Full/NDVI/ndvimmi.csv', delimiter=',')\n",
    "EMPbs = np.load('D:/Thesis/Notebooks/Output/Full/EMP/empbsi.npy')\n",
    "NDWIbs = loadtxt('D:/Thesis/Notebooks/Output/Full/NDWI/ndwibsi.csv', delimiter=',')\n",
    "NDVIbs = loadtxt('D:/Thesis/Notebooks/Output/Full/NDVI/ndvibsi.csv', delimiter=',')\n",
    "EMPbm = np.load('D:/Thesis/Notebooks/Output/Full/EMP/empbmi.npy')\n",
    "NDWIbm = loadtxt('D:/Thesis/Notebooks/Output/Full/NDWI/ndwibmi.csv', delimiter=',')\n",
    "NDVIbm = loadtxt('D:/Thesis/Notebooks/Output/Full/NDVI/ndvibmi.csv', delimiter=',')\n",
    "#Import Twitter\n",
    "Twita = rs.open('Input/Twitter/TwitterRaster_Amsterdam_fe.tif','r').read()\n",
    "Twita = np.transpose(Twita, [1, 2, 0])\n",
    "Twita = Twita[:,:,0]\n",
    "Twitm = rs.open('Input/Twitter//TwitterRaster_Milano.tif','r').read()\n",
    "Twitm = np.transpose(Twitm, [1, 2, 0])\n",
    "Twitm = Twitm[:,:,0]\n",
    "Twitb = rs.open('Input/Twitter/TwitterRaster_Budapest_fe.tif','r').read()\n",
    "Twitb = np.transpose(Twitb, [1, 2, 0])\n",
    "Twitb = Twitb[:,:,0]\n",
    "#Import Validation datasets\n",
    "Vala = rs.open('Input/Validation/Validation_30m_Amsterdam_fi.tif','r').read()\n",
    "Vala = np.transpose(Vala, [1, 2, 0])\n",
    "Val30ma = Vala[:,:,0]\n",
    "Val30ma = np.where(Val30ma==2, 1, 0)\n",
    "Val30ma = Val30ma.flatten()\n",
    "Val2a = rs.open('Input/Validation/Validation_GAIA_Amsterdam_fi.tif','r').read()\n",
    "Val2a = np.transpose(Val2a, [1, 2, 0])\n",
    "Val2GAIAa = Val2a[:,:,0]\n",
    "Val2GAIAa = np.where(Val2GAIAa>0, 1, 0)\n",
    "Val2GAIAa = Val2GAIAa.flatten()\n",
    "Val3a = rs.open('Input/Validation/Validation_GHS_Amsterdam_fi.tif', 'r').read()\n",
    "Val3a = np.transpose(Val3a, [1, 2, 0])\n",
    "Val3GHSa = Val3a[:,:,0]\n",
    "Val3GHS50a = np.where(Val3GHSa>50, 1, 0)\n",
    "Val3GHS50a = Val3GHS50a.flatten()\n",
    "\n",
    "Valm = rs.open('Input/Validation/Validation_30m_Milano_fi.tif','r').read()\n",
    "Valm = np.transpose(Valm, [1, 2, 0])\n",
    "Val30mm = Valm[:,:,0]\n",
    "Val30mm = np.where(Val30mm==2, 1, 0)\n",
    "Val30mm = Val30mm.flatten()\n",
    "Val2m = rs.open('Input/Validation/Validation_GAIA_Milano_fi.tif','r').read()\n",
    "Val2m = np.transpose(Val2m, [1, 2, 0])\n",
    "Val2GAIAm = Val2m[:,:,0]\n",
    "Val2GAIAm = np.where(Val2GAIAm>0, 1, 0)\n",
    "Val2GAIAm = Val2GAIAm.flatten()\n",
    "Val3m = rs.open('Input/Validation/Validation_GHS_Milano_fi.tif', 'r').read()\n",
    "Val3m = np.transpose(Val3m, [1, 2, 0])\n",
    "Val3GHSm = Val3m[:,:,0]\n",
    "Val3GHS50m = np.where(Val3GHSm>50, 1, 0)\n",
    "Val3GHS50m = Val3GHS50m.flatten()\n",
    "\n",
    "Valb = rs.open('Input/Validation/Validation_30m_Budapest_fi.tif','r').read()\n",
    "Valb = np.transpose(Valb, [1, 2, 0])\n",
    "Val30mb = Valb[:,:,0]\n",
    "Val30mb = np.where(Val30mb==2, 1, 0)\n",
    "Val30mb = Val30mb.flatten()\n",
    "Val2b = rs.open('Input/Validation/Validation_GAIA_Budapest_fi.tif','r').read()\n",
    "Val2b = np.transpose(Val2b, [1, 2, 0])\n",
    "Val2GAIAb = Val2b[:,:,0]\n",
    "Val2GAIAb = np.where(Val2GAIAb>0, 1, 0)\n",
    "Val2GAIAb = Val2GAIAb.flatten()\n",
    "Val3b = rs.open('Input/Validation/Validation_GHS_Budapest_fi.tif', 'r').read()\n",
    "Val3b = np.transpose(Val3b, [1, 2, 0])\n",
    "Val3GHSb = Val3b[:,:,0]\n",
    "Val3GHS50b = np.where(Val3GHSb>50, 1, 0)\n",
    "Val3GHS50b = Val3GHS50b.flatten()\n",
    "from numpy import *\n",
    "#GLCM_ASI_load = loadtxt('D:/Thesis/Notebooks/Output/Full/GLCM/ASI/GLCM_ASI.csv', delimiter=',')\n",
    "#img = read_data_EMP(asi)\n",
    "#nr, nc, nb = img.shape\n",
    "#nf = 16\n",
    "#GLCMas = GLCM_ASI_load.reshape(nr,nc,nf)\n",
    "#where_are_NaNs = isnan(GLCMas)\n",
    "#GLCMas[where_are_NaNs] = 0\n",
    "#GLCM_MSI_load = loadtxt('D:/Thesis/Notebooks/Output/Full/GLCM/MSI/GLCM_MSI.csv', delimiter=',')\n",
    "#img = read_data_EMP(msi)\n",
    "#nr, nc, nb = img.shape\n",
    "#nf = 16\n",
    "#GLCMms = GLCM_MSI_load.reshape(nr,nc,nf)\n",
    "#where_are_NaNs = isnan(GLCMms)\n",
    "#GLCMms[where_are_NaNs] = 0\n",
    "GLCM_BSI_load = loadtxt('D:/Thesis/Notebooks/Output/Full/GLCM/BSI/GLCM_BSI.csv', delimiter=',')\n",
    "img = read_data_EMP(bsi)\n",
    "nr, nc, nb = img.shape\n",
    "nf = 16\n",
    "GLCMbs = GLCM_BSI_load.reshape(nr,nc,nf)\n",
    "where_are_NaNs = isnan(GLCMbs)\n",
    "GLCMbs[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1541db5",
   "metadata": {},
   "source": [
    "## Run parameter optimalization\n",
    "\n",
    "parameters to crossreference\n",
    "\n",
    "IF - N_estimators & Threshold classmap\n",
    "\n",
    "SVM - Kernel & C & Threshold classmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#IF\n",
    "estimators = [50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
    "threshold = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "AOscores = {}\n",
    "Pscores = {}\n",
    "Rscores ={}\n",
    "Fscores = {}\n",
    "AUCscores = {}\n",
    "\n",
    "\n",
    "for e in estimators:\n",
    "    for t in threshold:\n",
    "        x = str(e+t)\n",
    "        img = read_data_EMP(asi)\n",
    "        stack = np.concatenate((img, EMPas, GLCMas), axis=2)\n",
    "        stacked = np.dstack((stack, NDVIas, NDWIas))\n",
    "        #Setup for IF and OCSVM\n",
    "        X = stacked\n",
    "        Ytr1 = Twita\n",
    "        nr,nc,nb = X.shape\n",
    "        ns = nr*nc\n",
    "        X = X.reshape((ns,nb))\n",
    "        Ytr = Ytr1.reshape((ns,))\n",
    "        ind = np.where(Ytr > 0)\n",
    "        Xtr = X[ind[0],:]\n",
    "        Ytr = Ytr[ind[0]]\n",
    "        standard_scaler = StandardScaler()\n",
    "        Xtr = standard_scaler.fit_transform(Xtr) \n",
    "        X = standard_scaler.transform(X)\n",
    "        model = IsolationForest(n_estimators = e)\n",
    "        model.fit(Xtr)\n",
    "        yhat = model.score_samples(X)\n",
    "        yhat1 = (yhat-min(yhat))/(max(yhat)-min(yhat))\n",
    "        class_map1 = np.reshape(yhat1,(nr,nc))\n",
    "        class_map = np.where(class_map1>t, 1, 0)\n",
    "        class_map = class_map.flatten()\n",
    "        pred = class_map\n",
    "        true1 = Val30ma\n",
    "        true2 = Val2GAIAa\n",
    "        true3 = Val3GHS50a\n",
    "        cfm1 = confusion_matrix(true1, pred)\n",
    "        TN1,FP1,FN1,TP1= cfm1.ravel()\n",
    "        cfm2 = confusion_matrix(true2, pred)\n",
    "        TN2,FP2,FN2,TP2= cfm2.ravel()\n",
    "        cfm3 = confusion_matrix(true3, pred)\n",
    "        TN3,FP3,FN3,TP3= cfm3.ravel()\n",
    "\n",
    "        AO = (((TP1+TN1)/(TP1+TN1+FP1+FN1))+\n",
    "              ((TP2+TN2)/(TP2+TN2+FP2+FN2))+\n",
    "              ((TP3+TN3)/(TP3+TN3+FP3+FN3)))/3\n",
    "        AOscores[x] = AO\n",
    "        \n",
    "        P = ((TP1/(TP1+FP1))+\n",
    "             (TP2/(TP2+FP2))+\n",
    "             (TP3/(TP3+FP3)))/3\n",
    "        Pscores[x] = P\n",
    "        \n",
    "        R = ((TP1/(TP1+FN1))+\n",
    "             (TP2/(TP2+FN2))+\n",
    "             (TP3/(TP3+FN3)))/3\n",
    "        Rscores[x] = R\n",
    "        \n",
    "        F = (((2*TP1)/(2*TP1+FP1+FN1))+\n",
    "             ((2*TP2)/(2*TP2+FP2+FN2))+\n",
    "             ((2*TP3)/(2*TP3+FP3+FN3)))/3\n",
    "        Fscores[x] = F\n",
    "        \n",
    "        auc1 = roc_auc_score(true1,pred)\n",
    "        auc2 = roc_auc_score(true2,pred)\n",
    "        auc3 = roc_auc_score(true3,pred)\n",
    "        score = ((auc1+auc2+auc3)/3)\n",
    "        AUCscores[x] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca57a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amsterdam\n",
    "all_values = AOscores.values()\n",
    "max_value = max(all_values)\n",
    "max_key = max(AOscores, key=AOscores.get)\n",
    "print('OA' + str(max_key) + str(max_value))\n",
    "\n",
    "all_values1 = Pscores.values()\n",
    "max_value1 = max(all_values1)\n",
    "max_key1 = max(Pscores, key=Pscores.get)\n",
    "print('P' + str(max_key1) + str(max_value1))\n",
    "\n",
    "all_values2 = Rscores.values()\n",
    "max_value2 = max(all_values2)\n",
    "max_key2 = max(Rscores, key=Rscores.get)\n",
    "print('R' + str(max_key2) + str(max_value2))\n",
    "\n",
    "all_values3 = Fscores.values()\n",
    "max_value3 = max(all_values3)\n",
    "max_key3 = max(Fscores, key=Fscores.get)\n",
    "print('F' + str(max_key3) + str(max_value3))\n",
    "\n",
    "all_values4 = AUCscores.values()\n",
    "max_value4 = max(all_values4)\n",
    "max_key4 = max(AUCscores, key=AUCscores.get)\n",
    "print('AUC' + str(max_key4) + str(max_value4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b21e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#IF\n",
    "estimators = [50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
    "threshold = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "AOscores = {}\n",
    "Pscores = {}\n",
    "Rscores ={}\n",
    "Fscores = {}\n",
    "AUCscores = {}\n",
    "\n",
    "\n",
    "for e in estimators:\n",
    "    for t in threshold:\n",
    "        x = str(e+t)\n",
    "        img = read_data_EMP(msi)\n",
    "        stack = np.concatenate((img, EMPms, GLCMms), axis=2)\n",
    "        stacked = np.dstack((stack, NDVIms, NDWIms))\n",
    "        #Setup for IF and OCSVM\n",
    "        X = stacked\n",
    "        Ytr1 = Twitm\n",
    "        nr,nc,nb = X.shape\n",
    "        ns = nr*nc\n",
    "        X = X.reshape((ns,nb))\n",
    "        Ytr = Ytr1.reshape((ns,))\n",
    "        ind = np.where(Ytr > 0)\n",
    "        Xtr = X[ind[0],:]\n",
    "        Ytr = Ytr[ind[0]]\n",
    "        standard_scaler = StandardScaler()\n",
    "        Xtr = standard_scaler.fit_transform(Xtr) \n",
    "        X = standard_scaler.transform(X)\n",
    "        model = IsolationForest(n_estimators = e)\n",
    "        model.fit(Xtr)\n",
    "        yhat = model.score_samples(X)\n",
    "        yhat1 = (yhat-min(yhat))/(max(yhat)-min(yhat))\n",
    "        class_map1 = np.reshape(yhat1,(nr,nc))\n",
    "        class_map = np.where(class_map1>t, 1, 0)\n",
    "        class_map = class_map.flatten()\n",
    "        pred = class_map\n",
    "        true1 = Val30mm\n",
    "        true2 = Val2GAIAm\n",
    "        true3 = Val3GHS50m\n",
    "        cfm1 = confusion_matrix(true1, pred)\n",
    "        TN1,FP1,FN1,TP1= cfm1.ravel()\n",
    "        cfm2 = confusion_matrix(true2, pred)\n",
    "        TN2,FP2,FN2,TP2= cfm2.ravel()\n",
    "        cfm3 = confusion_matrix(true3, pred)\n",
    "        TN3,FP3,FN3,TP3= cfm3.ravel()\n",
    "\n",
    "        AO = (((TP1+TN1)/(TP1+TN1+FP1+FN1))+\n",
    "              ((TP2+TN2)/(TP2+TN2+FP2+FN2))+\n",
    "              ((TP3+TN3)/(TP3+TN3+FP3+FN3)))/3\n",
    "        AOscores[x] = AO\n",
    "        \n",
    "        P = ((TP1/(TP1+FP1))+\n",
    "             (TP2/(TP2+FP2))+\n",
    "             (TP3/(TP3+FP3)))/3\n",
    "        Pscores[x] = P\n",
    "        \n",
    "        R = ((TP1/(TP1+FN1))+\n",
    "             (TP2/(TP2+FN2))+\n",
    "             (TP3/(TP3+FN3)))/3\n",
    "        Rscores[x] = R\n",
    "        \n",
    "        F = (((2*TP1)/(2*TP1+FP1+FN1))+\n",
    "             ((2*TP2)/(2*TP2+FP2+FN2))+\n",
    "             ((2*TP3)/(2*TP3+FP3+FN3)))/3\n",
    "        Fscores[x] = F\n",
    "        \n",
    "        auc1 = roc_auc_score(true1,pred)\n",
    "        auc2 = roc_auc_score(true2,pred)\n",
    "        auc3 = roc_auc_score(true3,pred)\n",
    "        score = ((auc1+auc2+auc3)/3)\n",
    "        AUCscores[x] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Milano\n",
    "all_values = AOscores.values()\n",
    "max_value = max(all_values)\n",
    "max_key = max(AOscores, key=AOscores.get)\n",
    "print('OA' + str(max_key) + str(max_value))\n",
    "\n",
    "all_values1 = Pscores.values()\n",
    "max_value1 = max(all_values1)\n",
    "max_key1 = max(Pscores, key=Pscores.get)\n",
    "print('P' + str(max_key1) + str(max_value1))\n",
    "\n",
    "all_values2 = Rscores.values()\n",
    "max_value2 = max(all_values2)\n",
    "max_key2 = max(Rscores, key=Rscores.get)\n",
    "print('R' + str(max_key2) + str(max_value2))\n",
    "\n",
    "all_values3 = Fscores.values()\n",
    "max_value3 = max(all_values3)\n",
    "max_key3 = max(Fscores, key=Fscores.get)\n",
    "print('F' + str(max_key3) + str(max_value3))\n",
    "\n",
    "all_values4 = AUCscores.values()\n",
    "max_value4 = max(all_values4)\n",
    "max_key4 = max(AUCscores, key=AUCscores.get)\n",
    "print('AUC' + str(max_key4) + str(max_value4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fdabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#IF\n",
    "estimators = [50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
    "threshold = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "AOscores = {}\n",
    "Pscores = {}\n",
    "Rscores ={}\n",
    "Fscores = {}\n",
    "AUCscores = {}\n",
    "\n",
    "\n",
    "for e in estimators:\n",
    "    for t in threshold:\n",
    "        x = str(e+t)\n",
    "        img = read_data_EMP(bsi)\n",
    "        stack = np.concatenate((img, EMPbs, GLCMbs), axis=2)\n",
    "        stacked = np.dstack((stack, NDVIbs, NDWIbs))\n",
    "        #Setup for IF and OCSVM\n",
    "        X = stacked\n",
    "        Ytr1 = Twitb\n",
    "        nr,nc,nb = X.shape\n",
    "        ns = nr*nc\n",
    "        X = X.reshape((ns,nb))\n",
    "        Ytr = Ytr1.reshape((ns,))\n",
    "        ind = np.where(Ytr > 0)\n",
    "        Xtr = X[ind[0],:]\n",
    "        Ytr = Ytr[ind[0]]\n",
    "        standard_scaler = StandardScaler()\n",
    "        Xtr = standard_scaler.fit_transform(Xtr) \n",
    "        X = standard_scaler.transform(X)\n",
    "        model = IsolationForest(n_estimators = e)\n",
    "        model.fit(Xtr)\n",
    "        yhat = model.score_samples(X)\n",
    "        yhat1 = (yhat-min(yhat))/(max(yhat)-min(yhat))\n",
    "        class_map1 = np.reshape(yhat1,(nr,nc))\n",
    "        class_map = np.where(class_map1>t, 1, 0)\n",
    "        class_map = class_map.flatten()\n",
    "        pred = class_map\n",
    "        true1 = Val30mb\n",
    "        true2 = Val2GAIAb\n",
    "        true3 = Val3GHS50b\n",
    "        cfm1 = confusion_matrix(true1, pred)\n",
    "        TN1,FP1,FN1,TP1= cfm1.ravel()\n",
    "        cfm2 = confusion_matrix(true2, pred)\n",
    "        TN2,FP2,FN2,TP2= cfm2.ravel()\n",
    "        cfm3 = confusion_matrix(true3, pred)\n",
    "        TN3,FP3,FN3,TP3= cfm3.ravel()\n",
    "\n",
    "        AO = (((TP1+TN1)/(TP1+TN1+FP1+FN1))+\n",
    "              ((TP2+TN2)/(TP2+TN2+FP2+FN2))+\n",
    "              ((TP3+TN3)/(TP3+TN3+FP3+FN3)))/3\n",
    "        AOscores[x] = AO\n",
    "        \n",
    "        P = ((TP1/(TP1+FP1))+\n",
    "             (TP2/(TP2+FP2))+\n",
    "             (TP3/(TP3+FP3)))/3\n",
    "        Pscores[x] = P\n",
    "        \n",
    "        R = ((TP1/(TP1+FN1))+\n",
    "             (TP2/(TP2+FN2))+\n",
    "             (TP3/(TP3+FN3)))/3\n",
    "        Rscores[x] = R\n",
    "        \n",
    "        F = (((2*TP1)/(2*TP1+FP1+FN1))+\n",
    "             ((2*TP2)/(2*TP2+FP2+FN2))+\n",
    "             ((2*TP3)/(2*TP3+FP3+FN3)))/3\n",
    "        Fscores[x] = F\n",
    "        \n",
    "        auc1 = roc_auc_score(true1,pred)\n",
    "        auc2 = roc_auc_score(true2,pred)\n",
    "        auc3 = roc_auc_score(true3,pred)\n",
    "        score = ((auc1+auc2+auc3)/3)\n",
    "        AUCscores[x] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e22951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Budapest\n",
    "all_values = AOscores.values()\n",
    "max_value = max(all_values)\n",
    "max_key = max(AOscores, key=AOscores.get)\n",
    "print('OA' + str(max_key) + str(max_value))\n",
    "\n",
    "all_values1 = Pscores.values()\n",
    "max_value1 = max(all_values1)\n",
    "max_key1 = max(Pscores, key=Pscores.get)\n",
    "print('P' + str(max_key1) + str(max_value1))\n",
    "\n",
    "all_values2 = Rscores.values()\n",
    "max_value2 = max(all_values2)\n",
    "max_key2 = max(Rscores, key=Rscores.get)\n",
    "print('R' + str(max_key2) + str(max_value2))\n",
    "\n",
    "all_values3 = Fscores.values()\n",
    "max_value3 = max(all_values3)\n",
    "max_key3 = max(Fscores, key=Fscores.get)\n",
    "print('F' + str(max_key3) + str(max_value3))\n",
    "\n",
    "all_values4 = AUCscores.values()\n",
    "max_value4 = max(all_values4)\n",
    "max_key4 = max(AUCscores, key=AUCscores.get)\n",
    "print('AUC' + str(max_key4) + str(max_value4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f77fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
